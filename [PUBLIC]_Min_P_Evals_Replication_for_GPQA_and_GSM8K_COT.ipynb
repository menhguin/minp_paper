{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/menhguin/minp_paper/blob/main/%5BPUBLIC%5D_Min_P_Evals_Replication_for_GPQA_and_GSM8K_COT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFLcmAxK7aYW"
      },
      "source": [
        "# LLM Evaluation\n",
        "\n",
        "In this notebook, we'll use the [language model evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness)\n",
        "utility built by EleutherAI to evaluate our model on a suite of different tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1. Install EleutherAI Evaluations Harness\n",
        "*   Logging into WandB is optional.\n",
        "*   Logging into Huggingface API is required to run GPQA. This is to prevent database leakage."
      ],
      "metadata": {
        "id": "qsKt8d6TVnC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import huggingface_hub\n",
        "from google.colab import userdata\n",
        "\n",
        "# Install latest versions of necessary libraries\n",
        "!pip install vllm==0.6.3  # For some reason, VLLM w eval harness breaks if you don't do this\n",
        "!pip install -e git+https://github.com/EleutherAI/lm-evaluation-harness.git#egg=lm_eval  # Install only lm-eval from source"
      ],
      "metadata": {
        "id": "S7G1cecrmr87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automated login for Hugging Face Hub via Colab Secrets. If you don't have this, it'll prompt for manual login if you don't have one. If you completely remove this, you can't run GPQA or use Llama models via HF."
      ],
      "metadata": {
        "id": "RxtRDlDfA_P7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for Huggingface API key and log in if available, otherwise prompt for manual login\n",
        "hf_token = userdata.get('HF_READ_TOKEN')\n",
        "if hf_token:\n",
        "    huggingface_hub.login(hf_token)\n",
        "else:\n",
        "    print(\"Huggingface token not found. Please login manually.\")\n",
        "    !huggingface-cli login"
      ],
      "metadata": {
        "id": "WKZucZXIA97D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automated login for WandB via Colab Secrets. If you don't have this, it'll just prompt you later if you use wandb_args."
      ],
      "metadata": {
        "id": "BBpgeHaLStRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for WandB API key and log in if available, otherwise skip login\n",
        "wandb_token = userdata.get('WANDB_API_KEY')\n",
        "if wandb_token:\n",
        "    os.environ[\"WANDB_API_KEY\"] = wandb_token\n",
        "    import wandb\n",
        "    wandb.login()\n",
        "else:\n",
        "    print(\"WandB token not found. Continuing without logging into WandB.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzNKgAPISUwK",
        "outputId": "278aab88-336c-4fb6-f85e-c7f64f6f10e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mminh1228\u001b[0m (\u001b[33mmenhguin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2. Run selected evals\n",
        "Change parameters as preferred:\n",
        "\n",
        "*   **Top P:** Lower values are more selective. It is recommended to use Top P = 0.9-0.95. - *E.g. Top P = 0.9 means using the fewest tokens that make up 90% of the probability distribution, and the remaining ~10% is truncated.*\n",
        "*   **Min P:** Lower values are less selective. It is recommended to use Min P = 0.05-0.1. - *E.g. Min P = 0.1 means every token where P < 10% of P(most probable token) is truncated.*\n",
        "*   **Temperature scaling:** Usually only 0 to 1 produces coherent output with Top P, but Min P allows good outputs until 3-5!"
      ],
      "metadata": {
        "id": "XO2k7iaKWjbr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. GPQA Main Generative (5-shot)"
      ],
      "metadata": {
        "id": "dWMLyPpPWQSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = \"min_p\"\n",
        "sampler_value = \"0.1\"\n",
        "tasks = \"gpqa_main_generative_n_shot\"\n",
        "model = \"vllm\"\n",
        "model_args = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "num_fewshot = \"5\""
      ],
      "metadata": {
        "id": "Js1B5DdFeR0u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 1\n",
        "\n",
        "!lm_eval \\\n",
        "    --model {model} \\\n",
        "    --model_args pretrained={model_args},dtype=auto \\\n",
        "    --batch_size \"auto\" \\\n",
        "    --tasks {tasks} \\\n",
        "    --num_fewshot {num_fewshot} \\\n",
        "    --log_samples \\\n",
        "    --output_path ./lm-eval-output/ \\\n",
        "    --gen_kwargs {sampler}={sampler_value},temperature={temperature},do_sample=True \\\n",
        "    --wandb_args project=lm-eval-harness-integration,name={tasks}_{sampler}_{sampler_value}_temp_{temperature}_{model}_{model_args.replace('/', '_')} \\\n",
        "    --device cuda\n",
        "\n",
        "#remove wandb_args if you don't already have/don't want to use WandB"
      ],
      "metadata": {
        "id": "F4Xrw41nxz0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B1. GSM8K Chain-of-Thought (8-shot) - Loop\n",
        "I think you still have to run each of these at least once without the loop for the install to work."
      ],
      "metadata": {
        "id": "-qdo4fL_DXSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "sampler = \"top_p\"\n",
        "sampler_values = [\"0.9\", \"0.95\"]  # Add as many sampler values as you want\n",
        "tasks = [\"gsm8k_cot_llama\"]\n",
        "model = \"vllm\"\n",
        "num_fewshot = \"5\"\n",
        "model_args_list = [\"meta-llama/Llama-3.2-1B-Instruct\", \"meta-llama/Llama-3.2-3B-Instruct\", \"meta-llama/Llama-3.1-8B-Instruct\"]  # Add different model args here\n",
        "temperature_values = [0, 0.5, 0.7, 1, 1.5, 2, 3, 4, 5]\n",
        "\n",
        "# Note that gsm8k_cot_llama with apply_chat_template and fewshot_as_multiturn are needed to replicate llama 3 Instruct benchmarks"
      ],
      "metadata": {
        "id": "zeJm-AtqDXSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each combination of model_args, task, sampler_value, and temperature\n",
        "for model_args in model_args_list:\n",
        "    for task in tasks:\n",
        "        for sampler_value in sampler_values:\n",
        "            for temperature in temperature_values:\n",
        "                # Construct the command as a single string\n",
        "                command = f\"\"\"\n",
        "                lm_eval \\\\\n",
        "                --model {model} \\\\\n",
        "                --model_args pretrained={model_args},dtype=auto \\\\\n",
        "                --batch_size auto \\\\\n",
        "                --tasks {task} \\\\\n",
        "                --num_fewshot {num_fewshot} \\\\\n",
        "                --apply_chat_template \\\\\n",
        "                --fewshot_as_multiturn \\\\\n",
        "                --log_samples \\\\\n",
        "                --output_path ./lm-eval-output/ \\\\\n",
        "                --gen_kwargs {sampler}={sampler_value},temperature={temperature},do_sample=True \\\\\n",
        "                --wandb_args project=lm-eval-harness-integration,name={task}_{sampler}_{sampler_value}_temp_{temperature}_{model}_{model_args.replace('/', '_')} \\\\\n",
        "                --device cuda\n",
        "                \"\"\"\n",
        "                print(f\"Running command with model_args={model_args}, task={task}, sampler_value={sampler_value}, temperature={temperature}\")\n",
        "\n",
        "\n",
        "                # Execute the command\n",
        "                os.system(command)"
      ],
      "metadata": {
        "id": "YZ4M3CfEDXSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##B2. GSM8K Chain-of-Thought (8-shot) (Self Consistency)\n",
        "We do not recommend running these unless you either have 50-100x the compute to run the previous evals, or lower the question limit to ~10 via `--limit 10 \\`."
      ],
      "metadata": {
        "id": "JXI_Pcx7Yt74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = \"min _p\"\n",
        "sampler_value = \"0.1\"\n",
        "tasks = \"gsm8k_cot_self_consistency\"\n",
        "model = \"vllm\"\n",
        "num_fewshot = \"5\""
      ],
      "metadata": {
        "id": "cwLQg7oDd_Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "temperature = 1\n",
        "\n",
        "!lm_eval \\\n",
        "    --model {model} \\\n",
        "    --model_args pretrained=mistralai/Mistral-7B-v0.1,dtype=auto \\\n",
        "    --batch_size \"auto\" \\\n",
        "    --tasks {tasks} \\\n",
        "    --num_fewshot {num_fewshot} \\\n",
        "    --log_samples \\\n",
        "    --output_path ./lm-eval-output/ \\\n",
        "    --gen_kwargs {sampler}={sampler_value},temperature={temperature},do_sample=True \\\n",
        "    --limit 10 \\ #self-consistency can have a lot of runs, remove this at your peril\n",
        "    --wandb_args project=lm-eval-harness-integration,name={tasks}_{sampler}_{sampler_value}_temp_{temperature} \\\n",
        "    --device cuda\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "6V3y-8dTHmAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reference: EleutherAI Eval Harness task list\n",
        "For those curious to run other evals! Please note that Min P is currently only accessible for `generate_until` tasks. There is currently no easy way to index these tasks, I just Ctrl + F'd `generate_until` on the [EleutherAI Evals Harness Repo](https://github.com/EleutherAI/lm-evaluation-harness)."
      ],
      "metadata": {
        "id": "P2PSZgKYZfCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !lm-eval --tasks list"
      ],
      "metadata": {
        "id": "x-RGcFC-PclJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Alternate: Git Clone Method for EleutherAI Evaluations Harness\n",
        "An alternate way to sometimes get around Evals Harness installation issues."
      ],
      "metadata": {
        "id": "ddBZmkepVMvx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Re0eppYizJ1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
        "!pip install -e lm-evaluation-harness"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}