{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/menhguin/minp_paper/blob/main/%5BPUBLIC%5D_Min_P_Evals_Replication_for_GPQA_and_GSM8K_COT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFLcmAxK7aYW"
      },
      "source": [
        "# LLM Evaluation\n",
        "\n",
        "In this notebook, we'll use the [language model evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness)\n",
        "utility built by EleutherAI to evaluate our model on a suite of different tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1. Install EleutherAI Evaluations Harness\n",
        "*   Logging into WandB is optional.\n",
        "*   Logging into Huggingface API is required to run GPQA. This is to prevent database leakage."
      ],
      "metadata": {
        "id": "qsKt8d6TVnC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e git+https://github.com/EleutherAI/lm-evaluation-harness.git#egg=lm_eval[wandb,vllm] # skip if you don't want to use wandb to log results\n",
        "!pip install lm_eval[wandb] # skip if you don't want to use wandb to log results\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "S7G1cecrmr87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2. Run selected evals\n",
        "Change parameters as preferred:\n",
        "\n",
        "*   **Top P:** Lower values are more selective. It is recommended to use Top P = 0.9-0.95. - *E.g. Top P = 0.9 means using the fewest tokens that make up 90% of the probability distribution, and the remaining ~10% is truncated.*\n",
        "*   **Min P:** Lower values are less selective. It is recommended to use Min P = 0.05-0.1. - *E.g. Min P = 0.1 means every token where P < 10% of P(most probable token) is truncated.*\n",
        "*   **Temperature scaling:** Usually only 0 to 1 produces coherent output with Top P, but Min P allows good outputs until 3-5!"
      ],
      "metadata": {
        "id": "XO2k7iaKWjbr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. GPQA Main Generative (5-shot)"
      ],
      "metadata": {
        "id": "dWMLyPpPWQSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = \"min_p\"\n",
        "sampler_value = \"0.1\"\n",
        "tasks = \"gpqa_main_generative_n_shot\"\n",
        "model = \"vllm\"\n",
        "num_fewshot = \"5\""
      ],
      "metadata": {
        "id": "Js1B5DdFeR0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 1\n",
        "\n",
        "!lm_eval \\\n",
        "    --model {model} \\\n",
        "    --model_args pretrained=mistralai/Mistral-7B-v0.1,dtype=auto \\\n",
        "    --batch_size \"auto\" \\\n",
        "    --tasks {tasks} \\\n",
        "    --num_fewshot {num_fewshot} \\\n",
        "    --log_samples \\\n",
        "    --output_path ./lm-eval-output/ \\\n",
        "    --gen_kwargs {sampler}={sampler_value},temperature={temperature},do_sample=True \\\n",
        "    --wandb_args project=lm-eval-harness-integration,name={tasks}_{sampler}_{sampler_value}_temp_{temperature} \\\n",
        "    --device cuda"
      ],
      "metadata": {
        "id": "F4Xrw41nxz0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B1. GSM8K Chain-of-Thought (8-shot)"
      ],
      "metadata": {
        "id": "DeKfQHU8Ywfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = \"min_p\"\n",
        "sampler_value = \"0.1\"\n",
        "tasks = \"gsm8k_cot\"\n",
        "model = \"vllm\"\n",
        "num_fewshot = \"8\""
      ],
      "metadata": {
        "id": "eseDynVWeJtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 1\n",
        "\n",
        "!lm_eval \\\n",
        "    --model {model} \\\n",
        "    --model_args pretrained=mistralai/Mistral-7B-v0.1,dtype=auto \\\n",
        "    --batch_size \"auto\" \\\n",
        "    --tasks {tasks} \\\n",
        "    --num_fewshot {num_fewshot} \\\n",
        "    --log_samples \\\n",
        "    --output_path ./lm-eval-output/ \\\n",
        "    --gen_kwargs {sampler}={sampler_value},temperature={temperature},do_sample=True \\\n",
        "    --wandb_args project=lm-eval-harness-integration,name={tasks}_{sampler}_{sampler_value}_temp_{temperature} \\\n",
        "    --device cuda"
      ],
      "metadata": {
        "id": "am-cJu-28kJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##B2. GSM8K Chain-of-Thought (8-shot) (Self Consistency)\n",
        "We do not recommend running these unless you either have 50-100x the compute to run the previous evals, or lower the question limit to ~10 via `--limit 10 \\`."
      ],
      "metadata": {
        "id": "JXI_Pcx7Yt74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = \"min _p\"\n",
        "sampler_value = \"0.1\"\n",
        "tasks = \"gsm8k_cot_self_consistency\"\n",
        "model = \"vllm\"\n",
        "num_fewshot = \"5\""
      ],
      "metadata": {
        "id": "cwLQg7oDd_Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "temperature = 1\n",
        "\n",
        "!lm_eval \\\n",
        "    --model {model} \\\n",
        "    --model_args pretrained=mistralai/Mistral-7B-v0.1,dtype=auto \\\n",
        "    --batch_size \"auto\" \\\n",
        "    --tasks {tasks} \\\n",
        "    --num_fewshot {num_fewshot} \\\n",
        "    --log_samples \\\n",
        "    --output_path ./lm-eval-output/ \\\n",
        "    --gen_kwargs {sampler}={sampler_value},temperature={temperature},do_sample=True \\\n",
        "    --limit 10 \\ #self-consistency can have a lot of runs, remove this at your peril\n",
        "    --wandb_args project=lm-eval-harness-integration,name={tasks}_{sampler}_{sampler_value}_temp_{temperature} \\\n",
        "    --device cuda\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "6V3y-8dTHmAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reference: EleutherAI Eval Harness task list\n",
        "For those curious to run other evals! Please note that Min P is currently only accessible for `generate_until` tasks. There is currently no easy way to index these tasks, I just Ctrl + F'd `generate_until` on the [EleutherAI Evals Harness Repo](https://github.com/EleutherAI/lm-evaluation-harness)."
      ],
      "metadata": {
        "id": "P2PSZgKYZfCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !lm-eval --tasks list"
      ],
      "metadata": {
        "id": "x-RGcFC-PclJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Alternate: Git Clone Method for EleutherAI Evaluations Harness\n",
        "An alternate way to sometimes get around Evals Harness installation issues."
      ],
      "metadata": {
        "id": "ddBZmkepVMvx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Re0eppYizJ1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
        "!pip install -e lm-evaluation-harness"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}